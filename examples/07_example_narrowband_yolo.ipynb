{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2acbdea-bdce-43f8-86d4-fb602f67beba",
   "metadata": {},
   "source": [
    "# Example 07 - TorchSig Narrowband with YOLOv8 Classifier\n",
    "This notebook showcases using the TorchSig Narrowband dataset to train a YOLOv8 classification model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2addcb1-6b3d-4484-a9b9-20d161840d95",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb874df6-3e3b-4fc0-a446-25f90923ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Training\n",
    "from torchsig.utils.yolo_classify import *\n",
    "from torchsig.utils.classify_transforms import real_imag_vstacked_cwt_image, complex_iq_to_heatmap\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810529c8-adf4-40d1-b735-455a72df2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for testing/inference\n",
    "from torchsig.datasets.modulations import ModulationsDataset\n",
    "from torchsig.datasets.signal_classes import torchsig_signals\n",
    "from torchsig.transforms.target_transforms import DescToFamilyName\n",
    "from torchsig.transforms.transforms import Spectrogram, SpectrogramImage, Normalize, Compose, Identity\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63eff7e-5dba-459b-88ba-d2e855c8cd27",
   "metadata": {},
   "source": [
    "## Prepare YOLO classificatoin trainer and Model\n",
    "Datasets are generated on the fly in a way that is Ultralytics YOLO compatible. See [Ultralytics: Train Custom Data - Organize Directories](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/#23-organize-directories) to learn more. \n",
    "\n",
    "Additionally, we create a yaml file for dataset configuration. See \"classify.yaml\" in Torchsig Examples.\n",
    "\n",
    "Download desired YOLO model from [Ultralytics Models](https://docs.ultralytics.com/models/). We will use YOLOv8, specifically `yolov8n-cls.pt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb6903-a7ec-4f3e-8555-8132cbfbc4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '07_yolo.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "overrides = config['overrides']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf456a-4d02-4ff8-b44c-f0b0c3014723",
   "metadata": {},
   "source": [
    "### Explanation of the `overrides` Dictionary\n",
    "\n",
    "The `overrides` dictionary is used to customize the settings for the Ultralytics YOLO trainer by specifying specific values that override the default configurations. The dictionary is imported from `classify.yaml`. However, you can customize in the notebook. \n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "overrides = {'model': 'yolov8n-cls.pt', 'epochs': 100, 'data': 'classify.yaml', 'device': 0, 'imgsz': 64}\n",
    "```\n",
    "A .yaml is necessary for training. Look at `classify.yaml` in the examples directory. It will contain the path to your torchsig data.\n",
    "\n",
    "### Explanation of `image_transform` function\n",
    "`YoloClassifyTrainer` allows you to pass in any transform that takes in complex I/Q and outputs an image for training. Some example transforms can be found in torchsig.utils.classify_transforms. If nothing is passed, it will default to spectrogram images. It is important to update `overrides` so that your imgsz matches output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset variables for yaml file\n",
    "config_name = \"07_yolo.yaml\"\n",
    "family_list = [\"ask\", \"fsk\", \"ofdm\", \"pam\", \"psk\", \"qam\"]\n",
    "family_dict = {v: k for v, k in enumerate(family_list)}\n",
    "classes = {v: k for v, k in enumerate(torchsig_signals.class_list)}\n",
    "num_classes = len(classes)\n",
    "yolo_root = \"./wideband/\" # train/val images (relative to './datasets``\n",
    "\n",
    "# define overrides\n",
    "overrides = dict(\n",
    "    model = \"yolov8n-cls.pt\",\n",
    "    project = \"yolo\",\n",
    "    name = \"07_example\",\n",
    "    epochs = 5,\n",
    "    imgsz = 512,\n",
    "    data = config_name,\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\",\n",
    "    batch = 32,\n",
    "    workers = 8\n",
    "\n",
    ")\n",
    "\n",
    "# create yaml file for trainer\n",
    "yolo_config = dict(\n",
    "    overrides = overrides,\n",
    "    train = yolo_root,\n",
    "    val = yolo_root,\n",
    "    level = 2,\n",
    "    include_snr = False,\n",
    "    num_samples = 530,\n",
    "    nc = num_classes,\n",
    "    names = classes,\n",
    "    family = False, # Determines if you are classify all 50+ classes or modulation family (see Classes below)\n",
    "    families = family_dict\n",
    ")\n",
    "\n",
    "with open(config_name, 'w+') as file:\n",
    "    yaml.dump(yolo_config, file, default_flow_style=False)\n",
    "\n",
    "print(f\"Creating experiment -> {overrides['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d94dba-1b87-4839-8c34-a2b449ede80d",
   "metadata": {},
   "source": [
    "### Build YoloClassifyTrainer\n",
    "This will instantiate the YOLO classification trainer with overrides specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134e461-456a-4be7-9c91-ce26c5c4f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = YoloClassifyTrainer(overrides=overrides, image_transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1cb68-5452-498b-bf5b-a7b96478429c",
   "metadata": {},
   "source": [
    "### Then begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d670a0-54e3-4a37-9649-740412e302ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e71d2-fc9c-4f60-9545-bb0997fb5334",
   "metadata": {},
   "source": [
    "### Instantiate Test Dataset\n",
    "\n",
    "Uses Torchsig's `ModulationsDataset` to generate a narrowband classification dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce307e25-99cb-4c72-9910-3d18d89d9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether to map descriptions to family names\n",
    "if config['family']:\n",
    "    target_transform = CP([DescToFamilyName()])\n",
    "else:\n",
    "    target_transform = None\n",
    "\n",
    "transform = Compose([\n",
    "    Spectrogram(nperseg=overrides['imgsz'], noverlap=0, nfft=overrides['imgsz'], mode='psd'),\n",
    "    Normalize(norm=np.inf, flatten=True),\n",
    "    SpectrogramImage(), \n",
    "    ])\n",
    "\n",
    "class_list = [item[1] for item in config['names'].items()]\n",
    "\n",
    "dataset = ModulationsDataset(\n",
    "    classes=class_list,\n",
    "    use_class_idx=False,\n",
    "    level=config['level'],\n",
    "    num_iq_samples=overrides['imgsz']**2,\n",
    "    num_samples=int(config['nc'] * 10),\n",
    "    include_snr=config['include_snr'],\n",
    "    transform=transform,\n",
    "    target_transform=target_transform\n",
    ")\n",
    "\n",
    "# Retrieve a sample and print out information\n",
    "idx = np.random.randint(len(dataset))\n",
    "data, label = dataset[idx]\n",
    "print(\"Dataset length: {}\".format(len(dataset)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(len(dataset))\n",
    "    sample, label = dataset[idx]\n",
    "    samples.append(sample)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab72b6-df31-4a08-88c7-b37400aec5d2",
   "metadata": {},
   "source": [
    "### Predictions / Inference\n",
    "The following cells show you how to load the 'best.pt' weights from your training for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644b3e9-a7c8-4865-8d16-f47b54cf7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(trainer.best) #The model will remember the configuration from training\n",
    "results = model.predict(samples, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135974bd-c0b2-478f-895b-96d487b9d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction results\n",
    "rows = 3\n",
    "cols = 3\n",
    "fig = plt.figure(figsize=(15, 15)) \n",
    "results_dir = results[0].save_dir\n",
    "\n",
    "for y, result in enumerate(results[:9]):\n",
    "    imgpath = os.path.join(results_dir, \"image\" + str(y) + \".jpg\")\n",
    "    fig.add_subplot(rows, cols, y + 1) \n",
    "    img = cv2.imread(imgpath)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Truth: \" + str(labels[y]), fontsize='large', loc='left')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
